{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-cognitiveservices-speech\n",
    "!pip install azure-identity\n",
    "!pip install openai\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "either endpoint, host, or region must be given along with a subscription key",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid you update the subscription info correctly?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# For English custom model with custom endpoint\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[43mAzure_TTS_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeech_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meng_custom_endpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQ9xEnglishxNeural\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHi, Q9!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# For Korean custom model with custom endpoint\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Azure_TTS_callback(speech_key, kor_custom_endpoint, \"TTSxMODELxNOSTYLENeural\", \"안녕하세요, Q9!\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 17\u001b[0m, in \u001b[0;36mAzure_TTS_callback\u001b[1;34m(speech_key, endpoint_url, voice_name, input_text)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mAzure_TTS_callback\u001b[39m(speech_key, endpoint_url, voice_name, input_text):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Set up the speech configuration using your subscription key and custom endpoint URL\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     speech_config \u001b[38;5;241m=\u001b[39m \u001b[43mspeechsdk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSpeechConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubscription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeech_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Assign the custom voice model by its name\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     speech_config\u001b[38;5;241m.\u001b[39mspeech_synthesis_voice_name \u001b[38;5;241m=\u001b[39m voice_name  \u001b[38;5;66;03m# Azure Speech portal\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\azure\\cognitiveservices\\speech\\speech.py:63\u001b[0m, in \u001b[0;36mSpeechConfig.__init__\u001b[1;34m(self, subscription, region, endpoint, host, auth_token, speech_recognition_language)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meither subscription key or authorization token must be given along with a region\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subscription \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m endpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m region \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meither endpoint, host, or region must be given along with a subscription key\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     65\u001b[0m generic_error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcannot construct SpeechConfig with the given arguments\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     66\u001b[0m c_subscription \u001b[38;5;241m=\u001b[39m _c_str(subscription)\n",
      "\u001b[1;31mValueError\u001b[0m: either endpoint, host, or region must be given along with a subscription key"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "if not load_dotenv() : \n",
    "    print(\"'.env' file is missing\")\n",
    "\n",
    "# Set your custom endpoints and subscription key here\n",
    "\n",
    "eng_custom_endpoint = os.getenv(\"ENG_ENDPOINT\")\n",
    "kor_custom_endpoint = os.getenv(\"KOR_ENDPOINT\")\n",
    "speech_key = os.getenv(\"AZURE_SPEECH_KEY\")\n",
    "\n",
    "# Function to handle TTS with Azure Speech SDK and custom endpoints\n",
    "def Azure_TTS_callback(speech_key, endpoint_url, voice_name, input_text):\n",
    "    # Set up the speech configuration using your subscription key and custom endpoint URL\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, endpoint=endpoint_url)\n",
    "    \n",
    "    # Assign the custom voice model by its name\n",
    "    speech_config.speech_synthesis_voice_name = voice_name  # Azure Speech portal\n",
    "\n",
    "    # Create a speech synthesizer instance\n",
    "    speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "    # Synthesizes the input text into speech asynchronously\n",
    "    result = speech_synthesizer.speak_text_async(input_text).get()\n",
    "\n",
    "    # Check the result\n",
    "    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        print(f\"Speech synthesized to speaker for text: [{input_text}]\")\n",
    "    elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = result.cancellation_details\n",
    "        print(f\"Speech synthesis canceled: {cancellation_details.reason}\")\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error and cancellation_details.error_details:\n",
    "            print(f\"Error details: {cancellation_details.error_details}\")\n",
    "        print(\"Did you update the subscription info correctly?\")\n",
    "\n",
    "# For English custom model with custom endpoint\n",
    "Azure_TTS_callback(speech_key, eng_custom_endpoint, \"EnglishxNeural\", \"Hi, I'm AI voice agent.\")\n",
    "# For Korean custom model with custom endpoint\n",
    "Azure_TTS_callback(speech_key, kor_custom_endpoint, \"TTSxMODELxNOSTYLENeural\", \"안녕하세요, 저는 음성 어시스턴트 입니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
